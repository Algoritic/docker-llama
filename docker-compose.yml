version: '3.6'

services:
  llama:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PORT: 8000 # port to expose
        MODEL: llama-2-7b-chat.Q2_K.gguf # give the exact name of the model
        HF_REPO: TheBloke/Llama-2-7b-Chat-GGUF # provide hugging face repository
    ports:
     - 8000:8000
    container_name: llama